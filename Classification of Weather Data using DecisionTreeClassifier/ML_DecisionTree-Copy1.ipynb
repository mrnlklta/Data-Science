{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification of Weather Data using DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Weather Data Analysis\n",
    "\n",
    "In this notebook, we will use scikit-learn to perform a decision tree based classification of weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Pandas DataFrame from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./weather/daily_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Weather Data Description\n",
    "The file **daily_weather.csv** is a comma-separated file that contains weather data. Data was collected for a period of three years, from September 2011 to September 2014, to ensure that sufficient data for different seasons and weather conditions is captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['number', 'air_pressure_9am', 'air_temp_9am', 'avg_wind_direction_9am',\n",
       "       'avg_wind_speed_9am', 'max_wind_direction_9am', 'max_wind_speed_9am',\n",
       "       'rain_accumulation_9am', 'rain_duration_9am', 'relative_humidity_9am',\n",
       "       'relative_humidity_3pm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in **daily_weather.csv** captures weather data for a separate day.\n",
    "Sensor measurements from the weather station were captured at one-minute intervals. These measurements were then processed to generate values to describe daily weather. Since this dataset was created to classify low-humidity days vs. non-low-humidity days (that is, days with normal or high humidity), the variables included are weather measurements in the morning, with one measurement, namely relatively humidity, in the afternoon. The idea is to use the morning weather values to predict whether the day will be low-humidity or not based on the afternoon measurement of relative humidity.\n",
    "\n",
    "Each row, or sample, consists of the following variables:\n",
    "\n",
    "* **number:** unique number for each row\n",
    "* **air_pressure_9am:** air pressure averaged over a period from 8:55am to 9:04am (*Unit: hectopascals*)\n",
    "* **air_temp_9am:** air temperature averaged over a period from 8:55am to 9:04am (*Unit: degrees Fahrenheit*)\n",
    "* **air_wind_direction_9am:** wind direction averaged over a period from 8:55am to 9:04am (*Unit: degrees, with 0 means coming from the North, and increasing clockwise*)\n",
    "* **air_wind_speed_9am:** wind speed averaged over a period from 8:55am to 9:04am (*Unit: miles per hour*)\n",
    "* ** max_wind_direction_9am:** wind gust direction averaged over a period from 8:55am to 9:10am (*Unit: degrees, with 0 being North and increasing clockwise*)\n",
    "* **max_wind_speed_9am:** wind gust speed averaged over a period from 8:55am to 9:04am (*Unit: miles per hour*)\n",
    "* **rain_accumulation_9am:** amount of rain accumulated in the 24 hours prior to 9am (*Unit: millimeters*)\n",
    "* **rain_duration_9am:** amount of time rain was recorded in the 24 hours prior to 9am (*Unit: seconds*)\n",
    "* **relative_humidity_9am:** relative humidity averaged over a period from 8:55am to 9:04am (*Unit: percent*)\n",
    "* **relative_humidity_3pm:** relative humidity averaged over a period from 2:55pm to 3:04pm (*Unit: percent *)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>air_pressure_9am</th>\n",
       "      <th>air_temp_9am</th>\n",
       "      <th>avg_wind_direction_9am</th>\n",
       "      <th>avg_wind_speed_9am</th>\n",
       "      <th>max_wind_direction_9am</th>\n",
       "      <th>max_wind_speed_9am</th>\n",
       "      <th>rain_accumulation_9am</th>\n",
       "      <th>rain_duration_9am</th>\n",
       "      <th>relative_humidity_9am</th>\n",
       "      <th>relative_humidity_3pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>918.060000</td>\n",
       "      <td>74.822000</td>\n",
       "      <td>271.100000</td>\n",
       "      <td>2.080354</td>\n",
       "      <td>295.400000</td>\n",
       "      <td>2.863283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.420000</td>\n",
       "      <td>36.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>917.347688</td>\n",
       "      <td>71.403843</td>\n",
       "      <td>101.935179</td>\n",
       "      <td>2.443009</td>\n",
       "      <td>140.471548</td>\n",
       "      <td>3.533324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.328697</td>\n",
       "      <td>19.426597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>923.040000</td>\n",
       "      <td>60.638000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>17.067852</td>\n",
       "      <td>63.700000</td>\n",
       "      <td>22.100967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>14.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>920.502751</td>\n",
       "      <td>70.138895</td>\n",
       "      <td>198.832133</td>\n",
       "      <td>4.337363</td>\n",
       "      <td>211.203341</td>\n",
       "      <td>5.190045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.189102</td>\n",
       "      <td>12.742547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>921.160000</td>\n",
       "      <td>44.294000</td>\n",
       "      <td>277.800000</td>\n",
       "      <td>1.856660</td>\n",
       "      <td>136.500000</td>\n",
       "      <td>2.863283</td>\n",
       "      <td>8.9</td>\n",
       "      <td>14730.0</td>\n",
       "      <td>92.410000</td>\n",
       "      <td>76.740000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  air_pressure_9am  air_temp_9am  avg_wind_direction_9am  \\\n",
       "0       0        918.060000     74.822000              271.100000   \n",
       "1       1        917.347688     71.403843              101.935179   \n",
       "2       2        923.040000     60.638000               51.000000   \n",
       "3       3        920.502751     70.138895              198.832133   \n",
       "4       4        921.160000     44.294000              277.800000   \n",
       "\n",
       "   avg_wind_speed_9am  max_wind_direction_9am  max_wind_speed_9am  \\\n",
       "0            2.080354              295.400000            2.863283   \n",
       "1            2.443009              140.471548            3.533324   \n",
       "2           17.067852               63.700000           22.100967   \n",
       "3            4.337363              211.203341            5.190045   \n",
       "4            1.856660              136.500000            2.863283   \n",
       "\n",
       "   rain_accumulation_9am  rain_duration_9am  relative_humidity_9am  \\\n",
       "0                    0.0                0.0              42.420000   \n",
       "1                    0.0                0.0              24.328697   \n",
       "2                    0.0               20.0               8.900000   \n",
       "3                    0.0                0.0              12.189102   \n",
       "4                    8.9            14730.0              92.410000   \n",
       "\n",
       "   relative_humidity_3pm  \n",
       "0              36.160000  \n",
       "1              19.426597  \n",
       "2              14.460000  \n",
       "3              12.742547  \n",
       "4              76.740000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>air_pressure_9am</th>\n",
       "      <th>air_temp_9am</th>\n",
       "      <th>avg_wind_direction_9am</th>\n",
       "      <th>avg_wind_speed_9am</th>\n",
       "      <th>max_wind_direction_9am</th>\n",
       "      <th>max_wind_speed_9am</th>\n",
       "      <th>rain_accumulation_9am</th>\n",
       "      <th>rain_duration_9am</th>\n",
       "      <th>relative_humidity_9am</th>\n",
       "      <th>relative_humidity_3pm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>917.890000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169.200000</td>\n",
       "      <td>2.192201</td>\n",
       "      <td>196.800000</td>\n",
       "      <td>2.930391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.990000</td>\n",
       "      <td>51.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>111</td>\n",
       "      <td>915.290000</td>\n",
       "      <td>58.820000</td>\n",
       "      <td>182.600000</td>\n",
       "      <td>15.613841</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>29.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>177</td>\n",
       "      <td>915.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.300000</td>\n",
       "      <td>4.719943</td>\n",
       "      <td>189.900000</td>\n",
       "      <td>5.346287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.260000</td>\n",
       "      <td>46.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>262</td>\n",
       "      <td>923.596607</td>\n",
       "      <td>58.380598</td>\n",
       "      <td>47.737753</td>\n",
       "      <td>10.636273</td>\n",
       "      <td>67.145843</td>\n",
       "      <td>13.671423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.990876</td>\n",
       "      <td>16.461685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>277</td>\n",
       "      <td>920.480000</td>\n",
       "      <td>62.600000</td>\n",
       "      <td>194.400000</td>\n",
       "      <td>2.751436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.869906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.580000</td>\n",
       "      <td>54.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number  air_pressure_9am  air_temp_9am  avg_wind_direction_9am  \\\n",
       "16       16        917.890000           NaN              169.200000   \n",
       "111     111        915.290000     58.820000              182.600000   \n",
       "177     177        915.900000           NaN              183.300000   \n",
       "262     262        923.596607     58.380598               47.737753   \n",
       "277     277        920.480000     62.600000              194.400000   \n",
       "\n",
       "     avg_wind_speed_9am  max_wind_direction_9am  max_wind_speed_9am  \\\n",
       "16             2.192201              196.800000            2.930391   \n",
       "111           15.613841              189.000000                 NaN   \n",
       "177            4.719943              189.900000            5.346287   \n",
       "262           10.636273               67.145843           13.671423   \n",
       "277            2.751436                     NaN            3.869906   \n",
       "\n",
       "     rain_accumulation_9am  rain_duration_9am  relative_humidity_9am  \\\n",
       "16                     0.0                0.0              48.990000   \n",
       "111                    0.0                0.0              21.500000   \n",
       "177                    0.0                0.0              29.260000   \n",
       "262                    0.0                NaN              17.990876   \n",
       "277                    0.0                0.0              52.580000   \n",
       "\n",
       "     relative_humidity_3pm  \n",
       "16               51.190000  \n",
       "111              29.690000  \n",
       "177              46.500000  \n",
       "262              16.461685  \n",
       "277              54.030000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will not need number for each row, so we can clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del data['number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's drop null values using the pandas *dropna* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n"
     ]
    }
   ],
   "source": [
    "before_rows = data.shape[0]\n",
    "print(before_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064\n"
     ]
    }
   ],
   "source": [
    "after_rows = data.shape[0]\n",
    "print(after_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many rows dropped due to cleaning?\n",
    "before_rows - after_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to a Classification Task\n",
    "Binarize the relative_humidity_3pm to 0 or 1.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "5    1\n",
      "6    0\n",
      "7    1\n",
      "8    0\n",
      "9    1\n",
      "Name: high_humidity_label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "clean_data = data.copy()\n",
    "clean_data['high_humidity_label'] = (clean_data['relative_humidity_3pm'] > 24.99)*1\n",
    "print(clean_data['high_humidity_label'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the target in 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=clean_data[['high_humidity_label']].copy()\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    36.160000\n",
       "1    19.426597\n",
       "2    14.460000\n",
       "3    12.742547\n",
       "4    76.740000\n",
       "Name: relative_humidity_3pm, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data['relative_humidity_3pm'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_humidity_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   high_humidity_label\n",
       "0                    1\n",
       "1                    0\n",
       "2                    0\n",
       "3                    0\n",
       "4                    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 9am Sensor Signals as Features to Predict Humidity at 3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morning_features = ['air_pressure_9am','air_temp_9am','avg_wind_direction_9am','avg_wind_speed_9am',\n",
    "        'max_wind_direction_9am','max_wind_speed_9am','rain_accumulation_9am',\n",
    "        'rain_duration_9am']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = clean_data[morning_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_pressure_9am', 'air_temp_9am', 'avg_wind_direction_9am',\n",
       "       'avg_wind_speed_9am', 'max_wind_direction_9am', 'max_wind_speed_9am',\n",
       "       'rain_accumulation_9am', 'rain_duration_9am'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['high_humidity_label'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Test and Train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Phase:\n",
    "\n",
    "In the **training phase**, the learning algorithm uses the training data to adjust the model’s parameters to minimize errors.  At the end of the training phase, you get the trained model.\n",
    "\n",
    "In the **testing phase**, the trained model is applied to test data.  Test data is separate from the training data, and is previously unseen by the model.  The model is then evaluated on how it performs on the test data.  The goal in building a classifier model is to have the model perform well on training as well as test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.model_selection.train_test_split:\n",
    "Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "**Parameters**:\n",
    "* *test_size*(float, int, or None (default is None)): If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. If train size is also None, test size is set to 0.25.\n",
    "* *train_size*(float, int, or None (default is None)): If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n",
    "* *random_state*(int or RandomState): Pseudo-random number generator state used for random sampling.\n",
    "\n",
    "**Returns**: list containing train-test split of inputs.\n",
    "\n",
    "Link: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(type(X_train))\n",
    "#print(type(X_test))\n",
    "#print(type(y_train))\n",
    "#print(type(y_test))\n",
    "#print(X_train.head())\n",
    "#print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GridSearchCV to find the optimum parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM_ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\IBM_ADMIN\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.model_selection.ShuffleSplit:\n",
    "*ShuffleSplit(n_splits=10, test_size=’default’, train_size=None, random_state=None)*\n",
    "\n",
    "Random permutation cross-validator. Yields indices to split data into training and test sets.\n",
    "\n",
    "Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.\n",
    "\n",
    "**Parameters for version 0.17**:\n",
    "* *n*(int): Total number of elements in the dataset.\n",
    "* *n_iter*(int (default 10)): Number of re-shuffling & splitting iterations.\n",
    "* *test_size*(float (default 0.1), int, or None): If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size.\n",
    "* *train_size*(float, int, or None (default is None)): If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n",
    "* *random_state*(int or RandomState): Pseudo-random number generator state used for random sampling.\n",
    "\n",
    "**Parameters for version 0.19**:\n",
    "* *n_splits*(int, default 10): Number of re-shuffling & splitting iterations.\n",
    "* *test_size*(float, int, None, default=0.1): If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the is parameter unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if train_size is unspecified, otherwise it will complement the specified train_size.\n",
    "* *train_size*(float, int, or None, default=None): If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n",
    "* *random_state*(int, RandomState instance or None, optional (default=None)): If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.metrics.make_scorer:\n",
    "\n",
    "*make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)*\n",
    "\n",
    "Make a scorer from a performance metric or loss function. This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as accuracy_score, mean_squared_error, adjusted_rand_index or average_precision and returns a callable that scores an estimator’s output.\n",
    "\n",
    "**Parameters**:\n",
    "* *score_func*(callable): Score function (or loss function) with signature score_func(y, y_pred, \\*\\*kwargs).\n",
    "* *greater_is_better*(boolean, default=True): Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func.\n",
    "* *needs_proba*(boolean, default=False): Whether score_func requires predict_proba to get probability estimates out of a classifier.\n",
    "* *needs_threshold*(boolean, default=False): Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. For example average_precision or the area under the roc curve can not be computed using discrete predictions alone.\n",
    "* *\\**kwargs*(additional arguments): Additional parameters to be passed to score_func.\n",
    "\n",
    "**Returns**:\t\n",
    "* *scorer*(callable): Callable object that returns a scalar score; greater is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimator to be used in GridSearchCV. All the fixed values of the parameters needs to be mentioned here.\n",
    "dct=DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "#Cross-validation to be used in GridSearchCV\n",
    "cv_sets = ShuffleSplit(X_train.shape[0], n_iter = 10, test_size = 0.20, random_state = 0) \n",
    "\n",
    "#Parameters to be searched in GridSearchCV\n",
    "params = {'max_leaf_nodes': list(range(2,26)), 'max_depth':list(range(2,26))}\n",
    "\n",
    "#Scoring function to be used in GridSearchCV.\n",
    "#We can either use make_scorer like the following, or we can directly use scoring='accuracy' in GridSearchCV\n",
    "def performance_metric(y_true, y_predict):\n",
    "    score = accuracy_score(y_true, y_predict)\n",
    "    return score\n",
    "scoring_fnc = make_scorer(performance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(712, n_iter=10, test_size=0.2, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(cv_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.model_selection.GridSearchCV\n",
    "\n",
    "*GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=True)*\n",
    "\n",
    "Exhaustive search over specified parameter values for an estimator. Important members are fit, predict. GridSearchCV implements a \"fit\" and a \"score\" method. It also implements \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n",
    "\n",
    "**Parameters**:\n",
    "* ***estimator***(estimator object): This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed.\n",
    "* ***param_grid***(dict or list of dictionaries): Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.\n",
    "* ***scoring***(string, callable, list/tuple, dict or None, default: None): A single string (see [The scoring parameter: defining model evaluation rules](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)) or a callable (see [Defining your scoring strategy from metric functions](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring)) to evaluate the predictions on the test set. For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values. NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each. If None, the estimator’s default scorer (if available) is used.\n",
    "* ***cv***(int, cross-validation generator or an iterable, optional): Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "    * *None*, to use the default 3-fold cross validation.\n",
    "    * *integer*, to specify the number of folds in a (Stratified)KFold.\n",
    "    * *An object* to be used as a cross-validation generator.\n",
    "    * *An iterable* yielding train, test splits.\n",
    "\n",
    "    For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n",
    "\n",
    "**Attributes**:\n",
    "* *cv\\_results\\_*(dict of numpy (masked) ndarrays): A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame.\n",
    "* ***best\\_estimator\\_***(estimator or dict): Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False.\n",
    "* ***best\\_score\\_***(float): Mean cross-validated score of the best_estimator. For multi-metric evaluation, this is present only if refit is specified.\n",
    "* ***best\\_params\\_***(dict): Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is present only if refit is specified.\n",
    "* *scorer\\_*(function or a dict): Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated scoring dict which maps the scorer key to the scorer callable.\n",
    "* *n\\_splits\\_*(int): The number of cross-validation splits (folds/iterations).\n",
    "\n",
    "**Methods**:\n",
    "* ***fit(X[, y, groups])***: Run fit with all sets of parameters.\n",
    "* *get_params([deep])*: Get parameters for this estimator.\n",
    "* *predict(X)*: Call predict on the estimator with the best found parameters.\n",
    "* *predict_proba(X)*: Call predict_proba on the estimator with the best found parameters.\n",
    "* *score(X[, y])*: Returns the score on the given data, if the estimator has been refit.\n",
    "* *transform(X)*: Call transform on the estimator with the best found parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=dct, param_grid=params, scoring='accuracy', cv=cv_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=21,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76993006993007"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.73427, std: 0.02866, params: {'max_depth': 2, 'max_leaf_nodes': 2},\n",
       " mean: 0.71119, std: 0.02168, params: {'max_depth': 2, 'max_leaf_nodes': 3},\n",
       " mean: 0.71958, std: 0.03196, params: {'max_depth': 2, 'max_leaf_nodes': 4},\n",
       " mean: 0.74126, std: 0.03620, params: {'max_depth': 2, 'max_leaf_nodes': 5},\n",
       " mean: 0.73986, std: 0.03061, params: {'max_depth': 2, 'max_leaf_nodes': 6},\n",
       " mean: 0.74755, std: 0.03531, params: {'max_depth': 2, 'max_leaf_nodes': 7},\n",
       " mean: 0.76713, std: 0.03607, params: {'max_depth': 2, 'max_leaf_nodes': 8},\n",
       " mean: 0.76713, std: 0.03607, params: {'max_depth': 2, 'max_leaf_nodes': 9},\n",
       " mean: 0.76713, std: 0.03607, params: {'max_depth': 2, 'max_leaf_nodes': 10},\n",
       " mean: 0.76713, std: 0.03607, params: {'max_depth': 2, 'max_leaf_nodes': 11}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'max_leaf_nodes': 21}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KFold Cross Validation:\n",
    "\n",
    "Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. It is used to assess the predictive performance of the models and and to judge how they perform on test data.\n",
    "\n",
    "The motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset. Without cross validation we only have information on how does our model perform to our in-sample data. Ideally we would like to see how does the model perform when we have a new data in terms of accuracy of its predictions.\n",
    "\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.\n",
    "\n",
    "For classification problems, one typically uses stratified k-fold cross-validation, in which the folds are selected so that each fold contains roughly the same proportions of class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.model_selection.KFold(n_splits=3, shuffle=False, random_state=None)\n",
    "\n",
    "*KFold(n_splits=3, shuffle=False, random_state=None)*\n",
    "\n",
    "K-Folds cross-validator provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    "\n",
    "**Parameters**:\t\n",
    "* ***n_splits***(int, default=3): Number of folds. Must be at least 2.\n",
    "* *shuffle*(boolean, optional): Whether to shuffle the data before splitting into batches.\n",
    "* *random_state*(int, RandomState instance or None, optional, default=None): If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Used when shuffle == True.\n",
    "\n",
    "**Methods**:\n",
    "* *get_n_splits([X, y, groups])*: Returns the number of splitting iterations in the cross-validator\n",
    "* ***split(X[, y, groups])***: Generate indices to split data into training and test set. Returns training and testing set indices for that split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76407356191421716"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_humidity_classifier = DecisionTreeClassifier(max_depth=7, max_leaf_nodes=21, random_state=0)\n",
    "index_list = []\n",
    "\n",
    "for i, (X_train_train, X_train_test) in enumerate(kf.split(X_train, y_train)):  #enumerate allows us to loop over something and have an automatic counter. \n",
    "    #X_train_train and X_train_test will contain the index number for the respective splits.\n",
    "    #with .iloc, we will use the index values of X_train_train to fit to the model.\n",
    "    model_humidity_classifier.fit(X_train.iloc[X_train_train],y_train.iloc[X_train_train])\n",
    "    #then we will use the index values of X_train_test to find the score of the model\n",
    "    x = model_humidity_classifier.score(X_train.iloc[X_train_test],y_train.iloc[X_train_test])\n",
    "    #We will add the performance score of the model to a list\n",
    "    index_list.append(x)\n",
    "\n",
    "#We will convert the list to an array to find out the mean of the performance score\n",
    "index_array = np.array(index_list)\n",
    "index_array.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.tree.DecisionTreeClassifier\n",
    "#### Parameters:\n",
    "* **criterion**(string, optional (default=”gini”)): The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "* **max_depth**(int or None, optional (default=None)): The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **min_samples_split**(int, float, optional (default=2)): The minimum number of samples required to split an internal node. If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "* **min_samples_leaf**(int, float, optional (default=1)): The minimum number of samples required to be at a leaf node. If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "* **max_leaf_nodes**(int or None, optional (default=None)): Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "* **class_weight**(dict, list of dicts, “balanced” or None, optional (default=None)): Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "* **random_state**(int, RandomState instance or None, optional (default=None)): If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "* **min_impurity_split**(float, optional (default=1e-7)): Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "#### Methods:\n",
    "* **fit(X, y[, sample_weight, check_input, ...])**: Build a decision tree classifier from the training set (X, y).\n",
    "* **score(X, y[, sample_weight])**: Returns the mean accuracy on the given test data and labels\n",
    "* **predict(X[, check_input])**: Predict class or regression value for X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=7,\n",
       "            max_features=None, max_leaf_nodes=21,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_humidity_classifier = DecisionTreeClassifier(max_depth=7, max_leaf_nodes=21, random_state=0) #Used the .best_params_ value from GridSearchCV\n",
    "model_humidity_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8595505617977528"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_humidity_classifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.tree.tree.DecisionTreeClassifier"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_humidity_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model_humidity_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456     0\n",
       "845     0\n",
       "693     1\n",
       "259     1\n",
       "723     1\n",
       "224     1\n",
       "300     1\n",
       "442     0\n",
       "585     1\n",
       "1057    1\n",
       "Name: high_humidity_label, dtype: int32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test['high_humidity_label'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Accuracy of the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.metrics.accuracy_score\n",
    "* **y_true**: Ground truth (correct) labels.\n",
    "* **y_pred**: Predicted labels, as returned by a classifier.\n",
    "* **normalize**(default=True): If *False*, returns the number of correctly classified samples. If *True*, returns the fraction of correctly classified samples.\n",
    "* **Returns**: If normalize == True, return the correctly classified samples (float), else it returns the number of correctly classified samples (int). The best performance is 1 with normalize == True and the number of samples with normalize == False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80965909090909094"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true = y_test, y_pred = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
